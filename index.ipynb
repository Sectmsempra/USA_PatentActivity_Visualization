{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffaa3150",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T19:31:19.183566Z",
     "iopub.status.busy": "2025-12-05T19:31:19.183450Z",
     "iopub.status.idle": "2025-12-05T19:31:19.686852Z",
     "shell.execute_reply": "2025-12-05T19:31:19.686239Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/93rpm4x14fv6n998658ffrlc0000gn/T/ipykernel_10590/4113713486.py:13: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(html.text)[0].copy()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "USPTO_ALL_CBSA_URL = \"https://www.uspto.gov/web/offices/ac/ido/oeip/taf/cls_cbsa/allcbsa_gd.htm\"\n",
    "\n",
    "def scrape_uspto_cbsa_patents(year: int, out_csv: str = None) -> pd.DataFrame:\n",
    "    if year < 2000 or year > 2015:\n",
    "        raise ValueError(\"This USPTO CBSA table supports years 2000–2015 only (not 2020).\")\n",
    "\n",
    "    html = requests.get(USPTO_ALL_CBSA_URL, timeout=30)\n",
    "    html.raise_for_status()\n",
    "\n",
    "    df = pd.read_html(html.text)[0].copy()\n",
    "\n",
    "    # ✅ Normalize column names\n",
    "    df.columns = [str(c).strip().upper() for c in df.columns]\n",
    "\n",
    "    name_col = \"U.S. REGIONAL TITLE\"\n",
    "    year_col = str(year)\n",
    "\n",
    "    if name_col not in df.columns:\n",
    "        raise RuntimeError(f\"Name column not found. Columns: {df.columns.tolist()}\")\n",
    "    if year_col not in df.columns:\n",
    "        raise RuntimeError(f\"Year column {year_col} not found. Columns: {df.columns.tolist()}\")\n",
    "\n",
    "    out = df[[name_col, year_col]].rename(\n",
    "        columns={name_col: \"cbsa_name\", year_col: \"patent_count\"}\n",
    "    ).copy()\n",
    "\n",
    "    out[\"cbsa_name\"] = out[\"cbsa_name\"].astype(str).str.strip()\n",
    "    out[\"patent_count\"] = pd.to_numeric(out[\"patent_count\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    out[\"year\"] = year\n",
    "\n",
    "    # remove totals / blank rows\n",
    "    out = out[(out[\"cbsa_name\"] != \"\") & (out[\"cbsa_name\"].str.upper() != \"TOTAL\")].reset_index(drop=True)\n",
    "\n",
    "    if out_csv:\n",
    "        out.to_csv(out_csv, index=False)\n",
    "\n",
    "    return out\n",
    "\n",
    "# Create the DataFrame in-memory; avoid writing intermediate CSV here\n",
    "df_2015 = scrape_uspto_cbsa_patents(2015)\n",
    "# print(df_2015.head())\n",
    "# print(\"Rows:\", len(df_2015))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7a96c7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T19:31:19.688689Z",
     "iopub.status.busy": "2025-12-05T19:31:19.688492Z",
     "iopub.status.idle": "2025-12-05T19:31:19.702394Z",
     "shell.execute_reply": "2025-12-05T19:31:19.701967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cbsa_name  year  patent_count\n",
      "0  -- Subtotal --  2015        140929\n",
      "1   Abbeville, LA  2015             1\n",
      "2    Aberdeen, SD  2015             5\n",
      "3    Aberdeen, WA  2015             4\n",
      "4     Abilene, TX  2015             8\n",
      "Rows: 1021\n",
      "Patent count min/max: 0 140929\n",
      "Top 5 MSAs:\n",
      "                                              cbsa_name  year  patent_count\n",
      "0                                       -- Subtotal --  2015        140929\n",
      "897                                   TOTAL, ALL AREAS  2015        140928\n",
      "811                 San Jose-Sunnyvale-Santa Clara, CA  2015         14618\n",
      "809                  San Francisco-Oakland-Fremont, CA  2015          9732\n",
      "615  New York-Northern New Jersey-Long Island, NY-N...  2015          7754\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1) Load (prefer in-memory `df_2015` if available)\n",
    "try:\n",
    "    df = df_2015.copy()\n",
    "except NameError:\n",
    "    df = pd.read_csv(\"uspto_cbsa_patents_2015.csv\")\n",
    "\n",
    "# 2) Basic cleanup\n",
    "df = df.copy()\n",
    "df[\"cbsa_name\"] = df[\"cbsa_name\"].astype(str)\n",
    "\n",
    "# strip whitespace + collapse multiple spaces\n",
    "df[\"cbsa_name\"] = (\n",
    "    df[\"cbsa_name\"]\n",
    "    .str.strip()\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    ")\n",
    "\n",
    "# normalize dash characters (sometimes tables contain weird unicode dashes)\n",
    "df[\"cbsa_name\"] = df[\"cbsa_name\"].str.replace(\"–\", \"-\", regex=False).str.replace(\"—\", \"-\", regex=False)\n",
    "\n",
    "# 3) Remove junk rows (blank names, \"TOTAL\", etc.)\n",
    "df = df[df[\"cbsa_name\"].ne(\"\")]\n",
    "df = df[~df[\"cbsa_name\"].str.upper().eq(\"TOTAL\")]\n",
    "\n",
    "# 4) Ensure numeric patent_count\n",
    "df[\"patent_count\"] = pd.to_numeric(df[\"patent_count\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# remove negatives if any (shouldn't happen, but safe)\n",
    "df.loc[df[\"patent_count\"] < 0, \"patent_count\"] = 0\n",
    "\n",
    "# 5) Ensure year is int\n",
    "df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df = df.dropna(subset=[\"year\"])\n",
    "df[\"year\"] = df[\"year\"].astype(int)\n",
    "\n",
    "# 6) Create helper: extract state codes from the end (e.g., \"... , CA\" or \"... , MA-NH\")\n",
    "# This is useful later if you need QA or disambiguation.\n",
    "def extract_states(name: str):\n",
    "    # capture everything after last comma as the \"state part\"\n",
    "    parts = name.rsplit(\",\", 1)\n",
    "    if len(parts) < 2:\n",
    "        return None\n",
    "    state_part = parts[1].strip()\n",
    "    # keep only uppercase letters and hyphens\n",
    "    state_part = re.sub(r\"[^A-Z\\-]\", \"\", state_part.upper())\n",
    "    return state_part if state_part else None\n",
    "\n",
    "df[\"state_codes\"] = df[\"cbsa_name\"].apply(extract_states)\n",
    "\n",
    "# 7) De-duplicate names (should not be needed, but safe)\n",
    "# If duplicates exist, sum their patents.\n",
    "df = (\n",
    "    df.groupby([\"cbsa_name\", \"year\"], as_index=False)[\"patent_count\"]\n",
    "      .sum()\n",
    ")\n",
    "\n",
    "# 8) Quick sanity checks\n",
    "print(df.head())\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Patent count min/max:\", df[\"patent_count\"].min(), df[\"patent_count\"].max())\n",
    "print(\"Top 5 MSAs:\\n\", df.sort_values(\"patent_count\", ascending=False).head(5))\n",
    "\n",
    "# # 9) Save cleaned version\n",
    "# df.to_csv(\"uspto_cbsa_patents_2015_clean.csv\", index=False)\n",
    "# print(\"Saved: uspto_cbsa_patents_2015_clean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef2fa390",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T19:31:19.703914Z",
     "iopub.status.busy": "2025-12-05T19:31:19.703793Z",
     "iopub.status.idle": "2025-12-05T19:31:19.724479Z",
     "shell.execute_reply": "2025-12-05T19:31:19.724041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched rows: 262\n",
      "                               cbsa_name  patent_count\n",
      "0                         -- Subtotal --        140929\n",
      "1                          Abbeville, LA             1\n",
      "7    Aguadilla-Isabela-San Sebasti�n, PR             3\n",
      "11                    Albany-Lebanon, OR            41\n",
      "21                           Allegan, MI            26\n",
      "32                          Anderson, IN            33\n",
      "33                          Anderson, SC            45\n",
      "44                         Ashtabula, OH             6\n",
      "51    Atlanta-Sandy Springs-Marietta, GA          2143\n",
      "62                  Baltimore-Towson, MD           846\n",
      "68                           Bastrop, LA             1\n",
      "85                     Bennettsville, SC             0\n",
      "87                         Berlin, NH-VT             1\n",
      "92                 Birmingham-Hoover, AL           112\n",
      "98                Bloomington-Normal, IL            74\n",
      "103                 Boise City-Nampa, ID           722\n",
      "105                            Boone, IA            12\n",
      "108       Boston-Cambridge-Quincy, MA-NH          5949\n",
      "115             Bremerton-Silverdale, WA           907\n",
      "118      Bridgeport-Stamford-Norwalk, CT           636\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# --- Load your cleaned patents file ---\n",
    "pat = df.copy()\n",
    "\n",
    "# --- Load census gazetteer (tab-delimited) ---\n",
    "gaz = pd.read_csv(\"2023_Gaz_cbsa_national.txt\", sep=\"\\t\", dtype={\"GEOID\": str})\n",
    "\n",
    "# Keep only relevant columns, normalize\n",
    "gaz = gaz.rename(columns={\"GEOID\": \"cbsa\", \"NAME\": \"cbsa_name_ref\"})\n",
    "gaz[\"cbsa_name_ref\"] = gaz[\"cbsa_name_ref\"].astype(str).str.strip()\n",
    "\n",
    "# ---------------------------\n",
    "# Normalization helpers\n",
    "# ---------------------------\n",
    "def norm_name(s: str) -> str:\n",
    "    s = str(s).upper().strip()\n",
    "    s = s.replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "\n",
    "    # Remove common suffix differences:\n",
    "    # Datawrapper/Census often has \"Metro Area\" / \"Micro Area\", USPTO sometimes doesn't\n",
    "    s = s.replace(\" METROPOLITAN STATISTICAL AREA\", \"\")\n",
    "    s = s.replace(\" MICROPOLITAN STATISTICAL AREA\", \"\")\n",
    "    s = s.replace(\" METRO AREA\", \"\")\n",
    "    s = s.replace(\" MICRO AREA\", \"\")\n",
    "\n",
    "    # Remove periods and extra punctuation\n",
    "    s = re.sub(r\"[\\.]\", \"\", s)\n",
    "\n",
    "    return s\n",
    "\n",
    "pat[\"name_norm\"] = pat[\"cbsa_name\"].apply(norm_name)\n",
    "gaz[\"name_norm\"] = gaz[\"cbsa_name_ref\"].apply(norm_name)\n",
    "\n",
    "# ---------------------------\n",
    "# Join patents to CBSA codes\n",
    "# ---------------------------\n",
    "merged = pat.merge(\n",
    "    gaz[[\"cbsa\", \"cbsa_name_ref\", \"name_norm\"]],\n",
    "    on=\"name_norm\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Report unmatched rows (important!)\n",
    "unmatched = merged[merged[\"cbsa\"].isna()][[\"cbsa_name\", \"patent_count\"]]\n",
    "print(\"Unmatched rows:\", len(unmatched))\n",
    "print(unmatched.head(20))\n",
    "\n",
    "# Keep only matched for Datawrapper\n",
    "dw = merged.dropna(subset=[\"cbsa\"]).copy()\n",
    "\n",
    "# Ensure CBSA is string (Datawrapper likes text IDs)\n",
    "dw[\"cbsa\"] = dw[\"cbsa\"].astype(str).str.zfill(5)\n",
    "\n",
    "# Final columns for Datawrapper\n",
    "dw_final = dw[[\"cbsa\", \"cbsa_name\", \"patent_count\", \"year\"]].rename(\n",
    "    columns={\"cbsa_name\": \"msa_name\"}\n",
    ")\n",
    "\n",
    "# Optional: create a label column for top 5\n",
    "dw_final[\"rank\"] = dw_final[\"patent_count\"].rank(method=\"dense\", ascending=False).astype(int)\n",
    "dw_final[\"top5_label\"] = dw_final.apply(\n",
    "    lambda r: r[\"msa_name\"] if r[\"rank\"] <= 5 else \"\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d13a15f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T19:31:19.726062Z",
     "iopub.status.busy": "2025-12-05T19:31:19.725952Z",
     "iopub.status.idle": "2025-12-05T19:31:19.728988Z",
     "shell.execute_reply": "2025-12-05T19:31:19.728590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['cbsa_name', 'patent_count', 'year'], dtype='object')\n",
      "                                           cbsa_name  patent_count  year\n",
      "0                 San Jose-Sunnyvale-Santa Clara, CA         14618  2015\n",
      "1  New York-Northern New Jersey-Long Island, NY-N...          7754  2015\n",
      "2                  San Francisco-Oakland-Fremont, CA          9732  2015\n",
      "3               Los Angeles-Long Beach-Santa Ana, CA          6476  2015\n",
      "4                     Boston-Cambridge-Quincy, MA-NH          5949  2015\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    df = df_2015.copy()\n",
    "except NameError:\n",
    "    df = pd.read_csv(\"uspto_cbsa_patents_2015.csv\")\n",
    "print(df.columns)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c72e232",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T19:31:19.730484Z",
     "iopub.status.busy": "2025-12-05T19:31:19.730337Z",
     "iopub.status.idle": "2025-12-05T19:31:19.745175Z",
     "shell.execute_reply": "2025-12-05T19:31:19.744578Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# your data (prefer in-memory)\n",
    "try:\n",
    "    pat = df_2015.copy()\n",
    "except NameError:\n",
    "    pat = pd.read_csv(\"uspto_cbsa_patents_2015.csv\")\n",
    "\n",
    "# gazetteer (update path/filename as needed)\n",
    "gaz = pd.read_csv(\"2023_Gaz_cbsa_national.txt\", sep=\"\\t\", dtype={\"GEOID\": str})\n",
    "\n",
    "gaz = gaz.rename(columns={\"GEOID\": \"cbsa\", \"NAME\": \"cbsa_name_ref\"})\n",
    "gaz[\"cbsa_name_ref\"] = gaz[\"cbsa_name_ref\"].astype(str).str.strip()\n",
    "\n",
    "def norm(s):\n",
    "    s = str(s).upper().strip()\n",
    "    s = s.replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = re.sub(r\"\\.\", \"\", s)\n",
    "    return s\n",
    "\n",
    "pat[\"key\"] = pat[\"cbsa_name\"].apply(norm)\n",
    "gaz[\"key\"] = gaz[\"cbsa_name_ref\"].apply(norm)\n",
    "\n",
    "merged = pat.merge(gaz[[\"cbsa\", \"cbsa_name_ref\", \"key\"]], on=\"key\", how=\"left\")\n",
    "\n",
    "# Keep only matched rows\n",
    "dw = merged.dropna(subset=[\"cbsa\"]).copy()\n",
    "dw[\"cbsa\"] = dw[\"cbsa\"].astype(str).str.zfill(5)\n",
    "\n",
    "# Top-5 label column (for Datawrapper labels/tooltips)\n",
    "dw[\"rank\"] = dw[\"patent_count\"].rank(method=\"dense\", ascending=False).astype(int)\n",
    "dw[\"top5_label\"] = dw.apply(lambda r: r[\"cbsa_name\"] if r[\"rank\"] <= 5 else \"\", axis=1)\n",
    "\n",
    "dw_final = dw[[\"cbsa\", \"cbsa_name\", \"patent_count\", \"year\", \"top5_label\"]].rename(\n",
    "    columns={\"cbsa_name\": \"msa_name\"}\n",
    ")\n",
    "\n",
    "# keep `dw_final` in-memory for downstream steps; do not write intermediate CSV\n",
    "# dw_final.to_csv(\"datawrapper_msa_patents_2015.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1539a02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T19:31:19.746637Z",
     "iopub.status.busy": "2025-12-05T19:31:19.746538Z",
     "iopub.status.idle": "2025-12-05T19:31:19.939695Z",
     "shell.execute_reply": "2025-12-05T19:31:19.939227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches after fuzzy: 762 out of 1027\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from difflib import get_close_matches\n",
    "\n",
    "try:\n",
    "    pat = df_2015.copy()\n",
    "except NameError:\n",
    "    pat = pd.read_csv(\"uspto_cbsa_patents_2015.csv\")\n",
    "gaz = pd.read_csv(\"2023_Gaz_cbsa_national.txt\", sep=\"\\t\", dtype={\"GEOID\": str})\n",
    "gaz = gaz.rename(columns={\"GEOID\": \"cbsa\", \"NAME\": \"cbsa_name_ref\"})\n",
    "\n",
    "def norm(s):\n",
    "    s = str(s).upper().strip()\n",
    "    s = s.replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = re.sub(r\"\\.\", \"\", s)\n",
    "    # remove Census suffixes\n",
    "    s = s.replace(\" METRO AREA\", \"\")\n",
    "    s = s.replace(\" MICRO AREA\", \"\")\n",
    "    s = s.replace(\" METROPOLITAN STATISTICAL AREA\", \"\")\n",
    "    s = s.replace(\" MICROPOLITAN STATISTICAL AREA\", \"\")\n",
    "    return s.strip()\n",
    "\n",
    "pat[\"key\"] = pat[\"cbsa_name\"].apply(norm)\n",
    "gaz[\"key\"] = gaz[\"cbsa_name_ref\"].apply(norm)\n",
    "\n",
    "# exact match merge first\n",
    "merged = pat.merge(gaz[[\"cbsa\", \"cbsa_name_ref\", \"key\"]], on=\"key\", how=\"left\")\n",
    "\n",
    "# fuzzy match for remaining (best-effort)\n",
    "gaz_keys = gaz[\"key\"].tolist()\n",
    "gaz_lookup = dict(zip(gaz[\"key\"], gaz[\"cbsa\"]))\n",
    "\n",
    "def fuzzy_cbsa(pat_key):\n",
    "    matches = get_close_matches(pat_key, gaz_keys, n=1, cutoff=0.92)\n",
    "    return gaz_lookup[matches[0]] if matches else None\n",
    "\n",
    "mask = merged[\"cbsa\"].isna()\n",
    "merged.loc[mask, \"cbsa\"] = merged.loc[mask, \"key\"].apply(fuzzy_cbsa)\n",
    "\n",
    "print(\"Matches after fuzzy:\", merged[\"cbsa\"].notna().sum(), \"out of\", len(merged))\n",
    "\n",
    "# Keep matched, format CBSA, and add top5 label\n",
    "dw = merged.dropna(subset=[\"cbsa\"]).copy()\n",
    "dw[\"cbsa\"] = dw[\"cbsa\"].astype(str).str.zfill(5)\n",
    "\n",
    "dw[\"rank\"] = dw[\"patent_count\"].rank(method=\"dense\", ascending=False).astype(int)\n",
    "dw[\"top5_label\"] = dw.apply(lambda r: r[\"cbsa_name\"] if r[\"rank\"] <= 5 else \"\", axis=1)\n",
    "\n",
    "dw_final = dw[[\"cbsa\", \"cbsa_name\", \"patent_count\", \"year\", \"top5_label\"]].rename(\n",
    "    columns={\"cbsa_name\": \"msa_name\"}\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bf0f434",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T19:31:19.941404Z",
     "iopub.status.busy": "2025-12-05T19:31:19.941256Z",
     "iopub.status.idle": "2025-12-05T19:31:19.950096Z",
     "shell.execute_reply": "2025-12-05T19:31:19.949682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    cbsa                                      msa_name  patent_count  \\\n",
      "0  41940            San Jose-Sunnyvale-Santa Clara, CA         14618   \n",
      "1  41860             San Francisco-Oakland-Fremont, CA          9732   \n",
      "2  42660                   Seattle-Tacoma-Bellevue, WA          4739   \n",
      "3  33460       Minneapolis-St. Paul-Bloomington, MN-WI          3419   \n",
      "4  19100               Dallas-Fort Worth-Arlington, TX          3026   \n",
      "5  12420              Austin-Round Rock-San Marcos, TX          2700   \n",
      "6  37980   Philadelphia-Camden-Wilmington, PA-NJ-DE-MD          2357   \n",
      "7  47900  Washington-Arlington-Alexandria, DC-VA-MD-WV          2310   \n",
      "8  38900           Portland-Vancouver-Hillsboro, OR-WA          2163   \n",
      "9  39580                              Raleigh-Cary, NC          1535   \n",
      "\n",
      "                                top5_label  \n",
      "0       San Jose-Sunnyvale-Santa Clara, CA  \n",
      "1        San Francisco-Oakland-Fremont, CA  \n",
      "2              Seattle-Tacoma-Bellevue, WA  \n",
      "3  Minneapolis-St. Paul-Bloomington, MN-WI  \n",
      "4          Dallas-Fort Worth-Arlington, TX  \n",
      "5                                           \n",
      "6                                           \n",
      "7                                           \n",
      "8                                           \n",
      "9                                           \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    df = dw_final.copy()\n",
    "except NameError:\n",
    "    try:\n",
    "        df = df_final.copy()\n",
    "    except NameError:\n",
    "        df = pd.read_csv(\"datawrapper_msa_patents_2015.csv\", dtype={\"cbsa\": str})\n",
    "\n",
    "# keep only what Datawrapper needs\n",
    "df = df[[\"cbsa\", \"msa_name\", \"patent_count\", \"top5_label\"]].copy()\n",
    "\n",
    "# enforce clean formatting\n",
    "df[\"cbsa\"] = df[\"cbsa\"].str.strip().str.zfill(5)\n",
    "df[\"msa_name\"] = df[\"msa_name\"].astype(str).str.strip()\n",
    "df[\"top5_label\"] = df[\"top5_label\"].astype(str).fillna(\"\").str.strip()\n",
    "\n",
    "# ensure patent_count is integer\n",
    "df[\"patent_count\"] = pd.to_numeric(df[\"patent_count\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# optional: remove any duplicates by cbsa (keep max, or sum)\n",
    "df = df.groupby([\"cbsa\", \"msa_name\"], as_index=False)[\"patent_count\"].sum()\n",
    "\n",
    "# recompute top5_label after grouping (important!)\n",
    "df = df.sort_values(\"patent_count\", ascending=False).reset_index(drop=True)\n",
    "df[\"top5_label\"] = \"\"\n",
    "df.loc[:4, \"top5_label\"] = df.loc[:4, \"msa_name\"]\n",
    "\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42576824",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T19:31:19.951821Z",
     "iopub.status.busy": "2025-12-05T19:31:19.951700Z",
     "iopub.status.idle": "2025-12-05T19:31:19.954356Z",
     "shell.execute_reply": "2025-12-05T19:31:19.953892Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"cbsa\"] = df[\"cbsa\"].astype(str).str.replace(\",\", \"\").str.zfill(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d0479b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T19:31:19.955710Z",
     "iopub.status.busy": "2025-12-05T19:31:19.955584Z",
     "iopub.status.idle": "2025-12-05T19:31:19.957727Z",
     "shell.execute_reply": "2025-12-05T19:31:19.957210Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# keep minimal result in-memory for subsequent steps\n",
    "df_minimal = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12056279",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T19:31:19.959021Z",
     "iopub.status.busy": "2025-12-05T19:31:19.958906Z",
     "iopub.status.idle": "2025-12-05T19:31:19.961785Z",
     "shell.execute_reply": "2025-12-05T19:31:19.961360Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    df = df_minimal.copy()\n",
    "except NameError:\n",
    "    df = pd.read_csv(\"datawrapper_msa_patents_MINIMAL.csv\", dtype={\"cbsa\": str})\n",
    "\n",
    "# remove these 10 unmatched codes (replace list with the exact red codes you see)\n",
    "bad = [\"24180\",\"30580\",\"35340\",\"42580\",\"32060\",\"36180\",\"13860\",\"18340\",\"33380\",\"46580\"]\n",
    "\n",
    "df = df[~df[\"cbsa\"].isin(bad)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbbf4c16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T19:31:19.963242Z",
     "iopub.status.busy": "2025-12-05T19:31:19.963135Z",
     "iopub.status.idle": "2025-12-05T19:31:19.969559Z",
     "shell.execute_reply": "2025-12-05T19:31:19.968976Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "try:\n",
    "    df = df_minimal.copy()\n",
    "except NameError:\n",
    "    df = pd.read_csv(\"datawrapper_msa_patents_MINIMAL.csv\", dtype={\"cbsa\": str})\n",
    "\n",
    "def clean_msa_name(s):\n",
    "    s = str(s).strip()\n",
    "    s = s.replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    # ensure there's a space after commas\n",
    "    s = re.sub(r\",\\s*\", \", \", s)\n",
    "    # remove any accidental code stuck at end (5 digits)\n",
    "    s = re.sub(r\"\\s*\\d{5}\\s*$\", \"\", s)\n",
    "    return s\n",
    "\n",
    "df[\"msa_name\"] = df[\"msa_name\"].apply(clean_msa_name)\n",
    "df[\"top5_label\"] = df[\"top5_label\"].apply(clean_msa_name)\n",
    "\n",
    "# keep cleaned-names version in-memory\n",
    "df_cleaned_names = df.copy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b97bf908",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T19:31:19.970825Z",
     "iopub.status.busy": "2025-12-05T19:31:19.970721Z",
     "iopub.status.idle": "2025-12-05T19:31:19.973961Z",
     "shell.execute_reply": "2025-12-05T19:31:19.973407Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    df = df_cleaned_names.copy()\n",
    "except NameError:\n",
    "    df = pd.read_csv(\"datawrapper_msa_patents_MINIMAL_clean_names.csv\", dtype={\"cbsa\": str})\n",
    "\n",
    "short_map = {\n",
    "    \"San Jose-Sunnyvale-Santa Clara, CA\": \"San Jose, CA\",\n",
    "    \"San Francisco-Oakland-Fremont, CA\": \"San Francisco, CA\",\n",
    "    \"Seattle-Tacoma-Bellevue, WA\": \"Seattle, WA\",\n",
    "    \"Minneapolis-St. Paul-Bloomington, MN-WI\": \"Minneapolis–St. Paul, MN–WI\",\n",
    "    \"Dallas-Fort Worth-Arlington, TX\": \"Dallas–Fort Worth, TX\",\n",
    "}\n",
    "\n",
    "df[\"top5_short_label\"] = df[\"top5_label\"].map(short_map).fillna(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05b852bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T19:31:19.975261Z",
     "iopub.status.busy": "2025-12-05T19:31:19.975156Z",
     "iopub.status.idle": "2025-12-05T19:31:19.978984Z",
     "shell.execute_reply": "2025-12-05T19:31:19.978413Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# prefer in-memory cleaned names\n",
    "try:\n",
    "    df = df_cleaned_names.copy()\n",
    "except NameError:\n",
    "    df = pd.read_csv(\"datawrapper_msa_patents_MINIMAL_clean_names.csv\", dtype={\"cbsa\": str})\n",
    "\n",
    "# Replace actual NaN + literal \"nan\" with empty strings\n",
    "df[\"top5_label\"] = df[\"top5_label\"].fillna(\"\").astype(str)\n",
    "df.loc[df[\"top5_label\"].str.lower().eq(\"nan\"), \"top5_label\"] = \"\"\n",
    "\n",
    "# (Optional) also clean msa_name if needed\n",
    "df[\"msa_name\"] = df[\"msa_name\"].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7816bf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T19:31:19.980232Z",
     "iopub.status.busy": "2025-12-05T19:31:19.980135Z",
     "iopub.status.idle": "2025-12-05T19:31:19.983567Z",
     "shell.execute_reply": "2025-12-05T19:31:19.983010Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prefer in-memory cleaned names or minimal frames before reading a file\n",
    "try:\n",
    "    df = df_cleaned_names.copy()\n",
    "except NameError:\n",
    "    try:\n",
    "        df = df_minimal.copy()\n",
    "    except NameError:\n",
    "        df = pd.read_csv(\"datawrapper_msa_patents_TOP5labels_NO_NAN.csv\", dtype={\"cbsa\": str})\n",
    "\n",
    "short_map = {\n",
    "    \"San Jose-Sunnyvale-Santa Clara, CA\": \"San Jose, CA\",\n",
    "    \"San Francisco-Oakland-Fremont, CA\": \"San Francisco, CA\",\n",
    "    \"Seattle-Tacoma-Bellevue, WA\": \"Seattle, WA\",\n",
    "    \"Minneapolis-St. Paul-Bloomington, MN-WI\": \"Minneapolis–St. Paul, MN–WI\",\n",
    "    \"Dallas-Fort Worth-Arlington, TX\": \"Dallas–Fort Worth, TX\",\n",
    "}\n",
    "\n",
    "df[\"top5_label\"] = df[\"top5_label\"].map(short_map).fillna(\"\")\n",
    "\n",
    "# expose this intermediate for later fallbacks\n",
    "df_top5_no_nan = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abc6c2cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T19:31:19.984822Z",
     "iopub.status.busy": "2025-12-05T19:31:19.984730Z",
     "iopub.status.idle": "2025-12-05T19:31:19.990051Z",
     "shell.execute_reply": "2025-12-05T19:31:19.989604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned file with mismatched CBSAs removed. Rows: 752\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Choose the most-recent in-memory DataFrame available, fall back to files\n",
    "try:\n",
    "    df = df_top5_short.copy()\n",
    "except NameError:\n",
    "    try:\n",
    "        df = df_top5_no_nan.copy()\n",
    "    except NameError:\n",
    "        try:\n",
    "            df = df_cleaned_names.copy()\n",
    "        except NameError:\n",
    "            try:\n",
    "                df = df_minimal.copy()\n",
    "            except NameError:\n",
    "                try:\n",
    "                    df = dw_final.copy()\n",
    "                except NameError:\n",
    "                    df = pd.read_csv(\"datawrapper_msa_patents_TOP5labels_SHORT.csv\", dtype={\"cbsa\": str})\n",
    "\n",
    "bad = {\"24180\",\"30500\",\"35340\",\"42580\",\"32060\",\"36180\",\"13860\",\"18340\",\"33380\",\"46580\"}\n",
    "df = df[~df[\"cbsa\"].isin(bad)].copy()\n",
    "\n",
    "# FINAL OUTPUT: write only here\n",
    "df.to_csv(\"datawrapper_cleaned.csv\", index=False)\n",
    "print(\"Saved cleaned file with mismatched CBSAs removed. Rows:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baea2bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
